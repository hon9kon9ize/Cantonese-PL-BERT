{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d31f54",
   "metadata": {},
   "source": [
    "# Notebook for preprocessing Wikipedia (English) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb8ed4",
   "metadata": {},
   "source": [
    "### Initilizing phonemizer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_path = \"Configs/config_yue.yml\" # you can change it to anything else\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e64f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemize import phonemize\n",
    "import ToJyutping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d58c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"hon9kon9ize/bert-large-cantonese\") # you can use any other tokenizers if you want to\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['dataset_params']['tokenizer']) # you can use any other tokenizers if you want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phonemeizer(text):\n",
    "    return ToJyutping.get_jyutping(text)\n",
    "\n",
    "phonemize(\"hello ! 你好》啊嗎？加崙\", phonemeizer, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25417",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5ae16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.zh-yue\")['train'] # you can use other version of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db15bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ca2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"./wiki_phoneme\" # set up root directory for multiprocessor processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda t: phonemize(t['text'], phonemeizer, tokenizer), remove_columns=['text'], num_proc=16, cache_file_name=f\"{root_directory}/phonemized_dataset.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(config['data_folder'])\n",
    "\n",
    "print('Dataset saved to %s' % config['data_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(config['data_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = dataset.to_pandas()\n",
    "\n",
    "phoneme_vocab = []\n",
    "for char in df['phonemes'].explode().unique():\n",
    "    phoneme_vocab.append(char)\n",
    "    \n",
    "phoneme_vocab = sorted(list(set(phoneme_vocab)))\n",
    "    \n",
    "with open(f\"{root_directory}/phoneme_vocab.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(phoneme_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce886d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset size\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6f6f6",
   "metadata": {},
   "source": [
    "### Remove unneccessary tokens from the pre-trained tokenizer\n",
    "The pre-trained tokenizer contains a lot of tokens that are not used in our dataset, so we need to remove these tokens. We also want to predict the word in lower cases because cases do not matter that much for TTS. Pruning the tokenizer is much faster than training a new tokenizer from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cec407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_loader import FilePathDataset, build_dataloader\n",
    "\n",
    "file_data = FilePathDataset(dataset)\n",
    "loader = build_dataloader(file_data, num_workers=32, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7504eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = config['dataset_params']['word_separator']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e968e",
   "metadata": {},
   "source": [
    "### Test the dataset with dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9025e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import build_dataloader\n",
    "import yaml\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from text_utils import symbols\n",
    "\n",
    "config_path = \"Configs/config_yue.yml\" # you can change it to anything else\n",
    "config = yaml.safe_load(open(config_path))\n",
    "dataset = load_from_disk(config['data_folder'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['dataset_params']['tokenizer']) # you can use any other tokenizers if you want to\n",
    "train_loader = build_dataloader(dataset, batch_size=32, num_workers=0, dataset_config=config['dataset_params'])\n",
    "train_loader.token_maps = {}\n",
    "\n",
    "_, (words, labels, phonemes, input_lengths, masked_indices) = next(enumerate(train_loader))\n",
    "\n",
    "print(tokenizer.decode(words[0]))\n",
    "print([symbols[i] for i in labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe50d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-vits2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
